/*-----------------------------------------.---------------------------------.
| Filename: TestLearning.cpp               | Test Learning                   |
| Author  : Francis Maes                   |                                 |
| Started : 28/03/2010 12:31               |                                 |
`------------------------------------------/                                 |
                               |                                             |
                               `--------------------------------------------*/

#include <lbcpp/lbcpp.h>
#include "GeneratedCode/Data/Bio/Protein.lh"
#include "VariableSetModel.h"
using namespace lbcpp;

extern void declareProteinsClasses();
extern void declareVariableSetClasses();

class SecondaryStructureVariableSetExample : public VariableSetExample
{
public:
  SecondaryStructureVariableSetExample(ProteinPtr protein)
    : aminoAcidSequence(protein->getAminoAcidSequence()),
      positionSpecificScores(protein->getPositionSpecificScoringMatrix()),
      secondaryStructure(protein->getSecondaryStructureSequence(false))
    {}
  
  virtual VariableSetPtr getTargetVariables() const
    {return secondaryStructure;}

  virtual VariableSetPtr createInitialPrediction() const
  {
    LabelSequencePtr res = new SecondaryStructureSequence(false);
    res->setLength(secondaryStructure->getLength());
    return res;
  }

  static featureGenerator numberLogFeatures(double positiveNumber, double weight)
  {
    static const double clog2 = log10(2.0);
    static const double clog5 = log10(5.0);
    double l = log10(positiveNumber);
    double fl = floor(l);

    static const double minLog10 = -4.0;
    if (fl < minLog10)
      featureSense (0, weight); // very small number
    else
    {
      double frac = l - fl;
      double base = pow(10.0, fl);
      int fracCase;
      double valMin, valMax;
      if (frac < clog2)
        fracCase = 0, valMin = 0.0, valMax = clog2;
      else if (frac < clog5)
        fracCase = 1, valMin = clog2, valMax = clog5;
      else
        fracCase = 2, valMin = clog5, valMax = 1.0;

      int featureNumber = 1 + (int)(fl - minLog10) * fracCase;
      jassert(featureNumber > 0);
      featureSense ((size_t)featureNumber, (frac - valMin) / (valMax - valMin) * weight);
    }
  }

  static featureGenerator numberFeatures(double value, double weight)
  {
    if (value > 0)
      featureCall("positive") inline numberLogFeatures(value, weight);
    else if (value < 0)
      featureCall("negative") inline numberLogFeatures(-value, weight);
    else
      featureSense("nullValue", weight);
  }

  static featureGenerator percentFeatures(double percent, double weight)
  {
    jassert(percent >= 0.0 && percent <= 1.0);
    enum {valueCount = 10};
    double p = percent * valueCount;
    featureSense((size_t)p, (p - (int)p) * weight);
  }

  featureGenerator posAndLengthFeatures(size_t variableIndex, double weight) const
  {
    double length = (double)aminoAcidSequence->getNumVariables();
    featureCall("length") inline numberFeatures(length, weight);
    featureCall("position") inline numberFeatures((double)variableIndex / length, weight);
  }

  virtual featureGenerator getVariableFeatures(size_t variableIndex, VariableSetPtr prediction) const
  {
    size_t nbAALeft = SecondaryStructureVariableSetExample::nbAALeft;
    size_t nbAARight = SecondaryStructureVariableSetExample::nbAARight;
    
    featureCall inline posAndLengthFeatures(variableIndex, 1.0);

    // conjunctions with current amino acid
    featureCall(T("A") + String((int)aminoAcidSequence->getLabel(variableIndex))) inline posAndLengthFeatures(variableIndex, 1.0);

    /* conjunctions with current pssm entries
    size_t n = positionSpecificScores->getNumScores();
    for (size_t i = 0; i < n; ++i)
    {
      double score = positionSpecificScores->getScore(variableIndex, i);
      if (score)
        featureCall(T("SS") + String((int)i)) inline posAndLengthFeatures(variableIndex, score);
    }*/
    
    featureCall("a") inline aminoAcidSequence->windowFeatures(variableIndex, nbAALeft, nbAARight, true);
    
    // conjunctions of AA of variable sizes, centred around the current position
    /*for (size_t i = 1; i < 4; ++i)
    {
      int p1 = (int)variableIndex - (int)i;
      int p2 = (int)variableIndex + (int)i;
      String featureName;
      for (int j = p1; j <= p2; ++j)
      {
        if (j < 0 || j >= (int)aminoAcidSequence->getLength())
          featureName += "_";
        else
          featureName += String(aminoAcidSequence->getLabel(j));
      }
      featureSense(featureName);
    }*/

    featureCall("p") inline positionSpecificScores->windowFeatures(variableIndex, nbAALeft, nbAARight, true);
    if (prediction)
    {
      LabelSequencePtr predictedSecondaryStructure = prediction.dynamicCast<LabelSequence>();
      jassert(predictedSecondaryStructure);
      featureCall("pr") inline predictedSecondaryStructure->windowFeatures(variableIndex, nbAALeft, nbAARight, false);
    }
  }
  
  static void setWindowLength(size_t nbAALeft, size_t nbAARight)
  {
    SecondaryStructureVariableSetExample::nbAALeft = nbAALeft;
    SecondaryStructureVariableSetExample::nbAARight = nbAARight;
  }
  
  static String toStaticString()
  {
    return String("Feature: ")
    + String("\n    > AA at left: ") + String((int) nbAALeft)
    + String("\n    > AA at right: ") + String((int) nbAARight);
  }
      
  
private:
  LabelSequencePtr aminoAcidSequence;
  ScoreVectorSequencePtr positionSpecificScores;
  LabelSequencePtr secondaryStructure;

  static size_t nbAALeft;
  static size_t nbAARight;
};

size_t SecondaryStructureVariableSetExample::nbAALeft = 7;
size_t SecondaryStructureVariableSetExample::nbAARight = 7;

class ProteinToVariableSetExample : public ObjectFunction
{
public:
  virtual String getOutputClassName(const String& inputClassName) const
    {return T("VariableSetExample");}

  virtual ObjectPtr function(ObjectPtr object) const
  {
    ProteinPtr protein = object.dynamicCast<Protein>();
    jassert(protein);
    return new SecondaryStructureVariableSetExample(protein);
  }
};

StoppingCriterionPtr createLearningStoppingCriterion(size_t nbIteration)
{
  return maxIterationsStoppingCriterion(nbIteration);
}

using juce::Time;

class TestTrainingProgressCallback : public TrainingProgressCallback
{
public:
  TestTrainingProgressCallback(StoppingCriterionPtr stoppingCriterion, ObjectContainerPtr validationData, OutputStream& results)
    : trainAccuracy(0.0), testAccuracy(0.0), stoppingCriterion(stoppingCriterion), validationData(validationData), results(results) {}
  
  double trainAccuracy;
  double testAccuracy;
  int iterationNumber;
  double startTime;
  
  virtual void progressStart(const String& description)
  {
    startTime = Time::getMillisecondCounter() / 1000.0;
    iterationNumber = 0;
    std::cout << description << std::endl;
    stoppingCriterion->reset();
  }
  
  virtual bool trainingProgressStep(LearningMachinePtr m, ObjectContainerPtr trainingData)
  {
    VariableSetModelPtr model = m.dynamicCast<VariableSetModel>();
    jassert(model);
    
    std::cout << std::endl << "Iteration " << iterationNumber << " Time since begin: " << (Time::getMillisecondCounter() / 1000.0 - startTime) << std::endl;

    std::cout << "Evaluating on training data..." << std::flush;
    trainAccuracy = model->evaluate(trainingData);
    std::cout << " => " << trainAccuracy << std::endl;

    std::cout << "Evaluating on testing data..." << std::flush;
    testAccuracy = model->evaluate(validationData);
    std::cout << " => " << testAccuracy << std::endl;
    
    double time = Time::getMillisecondCounter() / 1000.0;
    results << iterationNumber << " " << trainAccuracy << " " << testAccuracy << " " << (time - startTime) << "\n";
    results.flush();
    ++iterationNumber;
    
    return !stoppingCriterion->shouldOptimizerStop(trainAccuracy);
  }

  virtual void progressEnd()
    {std::cout << "Training finished." << std::endl;}
    
private:
  StoppingCriterionPtr stoppingCriterion;
  ObjectContainerPtr validationData;
  OutputStream& results;
};

GradientBasedClassifierPtr createMaxentClassifier(StringDictionaryPtr labels, IterationFunctionPtr learningRate, double regularizer)
{
  GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
  GradientBasedClassifierPtr classifier = maximumEntropyClassifier(learner, labels);
  classifier->setL2Regularizer(regularizer);
  return classifier;
}

void usage(char* command) {
  std::cout << "Usage: " << command << " PROTEIN_DIR RESULT_FILE NB_FOLD FOLD_ID NB_ITER REGULARIZER ('Inv' INIT ITER | 'Lin' CONST) NB_AA_LEFT NB_AA_RIGHT ('CO' | 'SICA' NB_PASSES RANDOM DETERMINISTIC)" << std::endl;
}

int main(int argc, char** argv)
{
  if (argc < 8)
  {
    usage(argv[0]);
    return -1;
  }
  
  size_t nextArgument = 1;
  
  File cwd = File::getCurrentWorkingDirectory();
  File cb513Directory = cwd.getChildFile(String(argv[nextArgument++]));
  File resultsFile = cwd.getChildFile(String(argv[nextArgument++]));
  size_t nbFolds = atoi(argv[nextArgument++]);
  size_t targetFold = atoi(argv[nextArgument++]);
  size_t nbIteration = atoi(argv[nextArgument++]);
  double regularizer = atof(argv[nextArgument++]);
  
  std::cout << "Protein Directory: " << cb513Directory.getFullPathName() << std::endl;
  std::cout << "Result File: " << resultsFile.getFullPathName() << std::endl;
  std::cout << "Fold: " << (targetFold+1) << "/" << nbFolds << std::endl;
  std::cout << "Stopping criterion: " << nbIteration << " iterations" << std::endl;
  std::cout << "L2 Regularizer value: " << regularizer << std::endl;
    

  resultsFile.deleteFile();
  OutputStream* resultsFileStream = resultsFile.createOutputStream();
  if (!resultsFileStream)
  {
    std::cerr << "Beeeruh" << std::endl;
    return 2;
  }
  
  
  std::cout << "Learning Rate: ";
  IterationFunctionPtr learningRate = NULL;
  String argumentValue = String(argv[nextArgument++]);
  if (argumentValue == String("Inv"))
  {
    double initialValue = atof(argv[nextArgument++]);
    size_t decreaseIteration = atoi(argv[nextArgument++]);
    learningRate = invLinearIterationFunction(initialValue, decreaseIteration);
    std::cout << "invLinear(" << initialValue << ", " << decreaseIteration  << ")" << std::endl;
  }
  else if (argumentValue == String("Lin"))
  {
    double constant = atof(argv[nextArgument++]);
    learningRate = constantIterationFunction(constant);
    std::cout << "constant(" << constant << ")" << std::endl;
  }
  else
  {
    std::cout << "unknown value '" << argumentValue << "'" << std::endl;
    usage(argv[0]);
    return -2;
  }

  //------
  declareProteinsClasses();
  ObjectStreamPtr proteinsStream = directoryObjectStream(cb513Directory, T("*.protein"));
  
  
    
  size_t nbAALeft = atoi(argv[nextArgument++]);
  size_t nbAARight = atoi(argv[nextArgument++]);
  
  SecondaryStructureVariableSetExample::setWindowLength(nbAALeft, nbAARight);
  std::cout << SecondaryStructureVariableSetExample::toStaticString() << std::endl;

  ObjectStreamPtr examplesStream = proteinsStream->apply(new ProteinToVariableSetExample());
  ObjectContainerPtr examples = examplesStream->load()->randomize();
  StringDictionaryPtr labels = examples->getAndCast<VariableSetExample>(0)->getTargetVariables()->getVariablesDictionary();
  //------



  GradientBasedClassifierPtr classifier = createMaxentClassifier(labels, learningRate, regularizer);

  std::cout << "Model: ";
  VariableSetModelPtr model = NULL;
  argumentValue = String(argv[nextArgument++]);
  if (argumentValue == String("CO"))
  {
    model = independantClassificationVariableSetModel(classifier);
    std::cout << "Content Only" << std::endl;
  }
  else if(argumentValue == String("SICA"))
  {
    size_t nbPasses = atoi(argv[nextArgument++]);
    bool randomOrder = atoi(argv[nextArgument++]) == 1;
    bool deterministic = atoi(argv[nextArgument++]) == 1;
    
    model = simulatedIterativeClassificationVariableSetModel(classifier, nbPasses, randomOrder, deterministic);
    std::cout << "Simlated Iterative Classification Algorithm" << std::endl;
    std::cout << "    > Maximun passes: " << nbPasses << std::endl;
    std::cout << "    > Random order: " << randomOrder << std::endl;
    std::cout << "    > Deterministic: " << deterministic << std::endl;
  }
  else
  {
    std::cout << "unkonwn value '" << argumentValue << "'" << std::endl;
    usage(argv[0]);
    return -3;
  }
  std::cout << model->toString() << std::endl;

  //VariableSetModelPtr model =
      //independantClassificationVariableSetModel(createMaxentClassifier(labels, learningRate));
      //optimisticClassificationVariableSetModel(createMaxentClassifier(labels, learningRate))
      //iterativeClassificationVariableSetModel(createMaxentClassifier(labels), createMaxentClassifier(labels, learningRate));
  //    simulatedIterativeClassificationVariableSetModel(createMaxentClassifier(labels, learningRate));

    
  ObjectContainerPtr trainingData = examples->invFold(targetFold, nbFolds);
  ObjectContainerPtr testingData = examples->fold(targetFold, nbFolds);

  ReferenceCountedObjectPtr<TestTrainingProgressCallback> callback
    = new TestTrainingProgressCallback(createLearningStoppingCriterion(nbIteration), testingData, *resultsFileStream);
  model->trainBatch(trainingData, callback);
  
  std::cout << "Train Accuracy = " << callback->trainAccuracy << std::endl;
  std::cout << "Test Accuracy = " << callback->testAccuracy << std::endl;
  
  delete resultsFileStream;
 
  // Results: Prediction of SS3 / 7 folds CV
  
  // ---CO---
  //
  // AA(7)+PSSM(7) with 1/1 train iter => 71.30
  // AA(7)+PSSM(7) with 2/2 train iter => 72.01
  // AA(7)+PSSM(7) with 2/10 train iter => 72.29
 
  
  
  // ---ICA---
  // AA(7)+PSSM(7) + PR(1) with 2/2 train iter => 70.96
  // AA(7)+PSSM(7) + PR(2) with 2/2 train iter => 71.67
  // AA(7)+PSSM(7) + PR(5) with 2/2 train iter => 71.63
  
  // ---SICA---
  // AA(7)+PSSM(7) + PR(2) with 2/2 train iter, reg 0, maxpass 5 => 70.61
  // AA(7)+PSSM(7) + PR(2) with 2/10 train iter, reg 0.0001, maxpass 10
  
  // ---CO Results with only 1 folds over the seven, 10 training iteration, best test score---
  // AA(7)+PSSM(7) => 74.02
  // AA(7)+PSSM(7)+POS/LEN => 74.16
  // AA(7)+PSSM(7)+POS/LEN/CONJ(AA) => 74.19
  // AA(7)+PSSM(7)+POS%/LEN/CONJ(AA) => 74.07
  // AA(7)+PSSM(7)+POS/LEN/CONJ(AA)/CONJ(PSSM) => 74.00

  // ---Test Results on the whole dataset (Predition of SS3)---
  // AA(15) => 62.2
  // PSSM(15) => 72.1
  // AA(15)+PSSM(15) => 73.9
  // AA(15)+PSSM(15)+OPT(1) => 88.3
  // AA(15)+PSSM(15)+OPT(10) => 89.1
  // AA(15)+PSSM(15)+OPT_8(10) => 90.5
  // PSSM(15)+OPT(1) => 87.8
  // AA(15)+OPT(1) => 87.6
  // OPT(1) => 82.8
  // OPT(10) => 84.7
  return 0;
}
