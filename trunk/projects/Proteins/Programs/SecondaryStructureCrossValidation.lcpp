/*-----------------------------------------.---------------------------------.
| Filename: TestLearning.cpp               | Test Learning                   |
| Author  : Francis Maes                   |                                 |
| Started : 28/03/2010 12:31               |                                 |
`------------------------------------------/                                 |
                               |                                             |
                               `--------------------------------------------*/

#include <lbcpp/lbcpp.h>
#include "GeneratedCode/Data/Bio/Protein.lh"
#include "VariableSetModel.h"
using namespace lbcpp;

extern void declareProteinsClasses();
extern void declareInterdependantVariableSetClasses();

class SecondaryStructureVariableSetExample : public VariableSetExample
{
public:
  SecondaryStructureVariableSetExample(ProteinPtr protein)
    : aminoAcidSequence(protein->getAminoAcidSequence()),
      positionSpecificScores(protein->getPositionSpecificScoringMatrix()),
      secondaryStructure(protein->getSecondaryStructureSequence(false))
    {}
  
  virtual VariableSetPtr getTargetVariables() const
    {return secondaryStructure;}

  virtual VariableSetPtr createInitialPrediction() const
  {
    LabelSequencePtr res = new SecondaryStructureSequence(false);
    res->setLength(secondaryStructure->getLength());
    return res;
  }
    
    
  static featureGenerator numberLogFeaturesBase(const String& nameMin, const String& nameMax, double valMin, double valMax, double value, double weight)
  {
    jassert(value >= valMin && value <= valMax);
    double delta = valMax - valMin;
    jassert(delta > 0);
    featureSense (nameMin, (1 - ((value - valMin) / delta)) * weight);
    featureSense (nameMax, (1 - ((valMax - value) / delta)) * weight);
  }

  static featureGenerator numberLogFeatures(double positiveNumber, double weight)
  {
    static const double clog2 = log10(2.0);
    static const double clog5 = log10(5.0);
    double l = log10(positiveNumber);
    double fl = floor(l);
    double frac = l - fl;
    double base = pow(10.0, fl);

    if (frac < clog2)
      featureCall inline numberLogFeaturesBase(lbcpp::toString(base), lbcpp::toString(2 * base), 0, clog2, frac, weight);
    else if (frac < clog5)
      featureCall inline numberLogFeaturesBase(lbcpp::toString(2 * base), lbcpp::toString(5 * base), clog2, clog5, frac, weight);
    else
      featureCall inline numberLogFeaturesBase(lbcpp::toString(5 * base), lbcpp::toString(10 * base), clog5, 1.0, frac, weight);
  }

  static featureGenerator numberFeatures(double value, double weight)
  {
    if (value > 0)
      featureCall("positive") inline numberLogFeatures(value, weight);
    else if (value < 0)
      featureCall("negative") inline numberLogFeatures(-value, weight);
    else
      featureSense("nullValue", weight);
  }

  static featureGenerator percentFeatures(double percent, double weight)
  {
    jassert(percent >= 0.0 && percent <= 1.0);
    enum {valueCount = 10};
    double p = percent * valueCount;
    featureSense((size_t)p, (p - (int)p) * weight);
  }

  featureGenerator posAndLengthFeatures(size_t variableIndex, double weight) const
  {
    double length = (double)aminoAcidSequence->getNumVariables();
    featureCall("length") numberFeatures(length, weight);
    featureCall("position") numberFeatures((double)variableIndex / length, weight);
  }

  virtual featureGenerator getVariableFeatures(size_t variableIndex, VariableSetPtr prediction) const
  {
    featureCall inline posAndLengthFeatures(variableIndex, 1.0);

    // conjunctions with current amino acid
    featureCall(T("A") + String((int)aminoAcidSequence->getLabel(variableIndex))) inline posAndLengthFeatures(variableIndex, 1.0);

    /* conjunctions with current pssm entries
    size_t n = positionSpecificScores->getNumScores();
    for (size_t i = 0; i < n; ++i)
    {
      double score = positionSpecificScores->getScore(variableIndex, i);
      if (score)
        featureCall(T("SS") + String((int)i)) inline posAndLengthFeatures(variableIndex, score);
    }*/
    
    featureCall("a") aminoAcidSequence->windowFeatures(variableIndex, 7, 7, true);
    
    // conjunctions of AA of variable sizes, centred around the current position
    /*for (size_t i = 1; i < 4; ++i)
    {
      int p1 = (int)variableIndex - (int)i;
      int p2 = (int)variableIndex + (int)i;
      String featureName;
      for (int j = p1; j <= p2; ++j)
      {
        if (j < 0 || j >= (int)aminoAcidSequence->getLength())
          featureName += "_";
        else
          featureName += String(aminoAcidSequence->getLabel(j));
      }
      featureSense(featureName);
    }*/

    featureCall("p") positionSpecificScores->windowFeatures(variableIndex, 7, 7, true);
    if (prediction)
    {
      LabelSequencePtr predictedSecondaryStructure = prediction.dynamicCast<LabelSequence>();
      jassert(predictedSecondaryStructure);
      featureCall("pr") predictedSecondaryStructure->windowFeatures(variableIndex, 7, 7, false);
    }
  }
    
private:
  LabelSequencePtr aminoAcidSequence;
  ScoreVectorSequencePtr positionSpecificScores;
  LabelSequencePtr secondaryStructure;
};

class ProteinToVariableSetExample : public ObjectFunction
{
public:
  virtual String getOutputClassName(const String& inputClassName) const
    {return T("VariableSetExample");}

  virtual ObjectPtr function(ObjectPtr object) const
  {
    ProteinPtr protein = object.dynamicCast<Protein>();
    jassert(protein);
    return new SecondaryStructureVariableSetExample(protein);
  }
};

StoppingCriterionPtr createLearningStoppingCriterion()
{
  return maxIterationsStoppingCriterion(100);
}

using juce::Time;

class TestTrainingProgressCallback : public TrainingProgressCallback
{
public:
  TestTrainingProgressCallback(StoppingCriterionPtr stoppingCriterion, ObjectContainerPtr validationData, OutputStream& results)
    : trainAccuracy(0.0), testAccuracy(0.0), stoppingCriterion(stoppingCriterion), validationData(validationData), results(results) {}
  
  double trainAccuracy;
  double testAccuracy;
  int iterationNumber;
  double startTime;
  
  virtual void progressStart(const String& description)
  {
    startTime = Time::getMillisecondCounter() / 1000.0;
    iterationNumber = 0;
    std::cout << description << std::endl;
    stoppingCriterion->reset();
  }
  
  virtual bool trainingProgressStep(LearningMachinePtr m, ObjectContainerPtr trainingData)
  {
    VariableSetModelPtr model = m.dynamicCast<VariableSetModel>();
    jassert(model);
    
    std::cout << std::endl << "Iteration " << iterationNumber << " Time since begin: " << (Time::getMillisecondCounter() / 1000.0 - startTime) << std::endl;

    std::cout << "Evaluating on training data..." << std::flush;
    trainAccuracy = model->evaluate(trainingData);
    std::cout << " => " << trainAccuracy << std::endl;

    std::cout << "Evaluating on testing data..." << std::flush;
    testAccuracy = model->evaluate(validationData);
    std::cout << " => " << testAccuracy << std::endl;
    
    double time = Time::getMillisecondCounter() / 1000.0;
    results << iterationNumber << " " << trainAccuracy << " " << testAccuracy << " " << (time - startTime) << "\n";
    results.flush();
    ++iterationNumber;
    
    return !stoppingCriterion->shouldOptimizerStop(trainAccuracy);
  }

  virtual void progressEnd()
    {std::cout << "Training finished." << std::endl;}
    
private:
  StoppingCriterionPtr stoppingCriterion;
  ObjectContainerPtr validationData;
  OutputStream& results;
};

GradientBasedClassifierPtr createMaxentClassifier(StringDictionaryPtr labels)
{
  IterationFunctionPtr learningRate = constantIterationFunction(1.0);//InvLinear(26, 10000);
  GradientBasedLearnerPtr learner = 
    stochasticDescentLearner(learningRate);  
  GradientBasedClassifierPtr classifier = maximumEntropyClassifier(learner, labels);
  classifier->setL2Regularizer(0.0001);
  return classifier;
}

void usage(char* command) {
  std::cout << "Usage: " << command << " PROTEIN_DIR RESULT_FILE FOLD_NUM" << std::endl;
}

int main(int argc, char** argv)
{
  if (argc != 4) {
    usage(argv[0]);
    return 0;
  }
  
  std::cout << "Protein Directory: " << argv[1] << std::endl;
  std::cout << "Result File: " << argv[2] << std::endl;
  std::cout << "Fold: " << argv[3] << std::endl;
  
  File cwd = File::getCurrentWorkingDirectory();
  File cb513Directory = cwd.getChildFile(String(argv[1]));
  File resultsFile = cwd.getChildFile(String(argv[2]));
  int targetFold = atoi(argv[3]);
  
  resultsFile.deleteFile();
  OutputStream* resultsFileStream = resultsFile.createOutputStream();
  if (!resultsFileStream)
  {
    std::cerr << "Beeeruh" << std::endl;
    return 2;
  }
  
  
  declareProteinsClasses();
  ObjectStreamPtr proteinsStream = directoryObjectStream(cb513Directory, T("*.protein"));
  ObjectStreamPtr examplesStream = proteinsStream->apply(new ProteinToVariableSetExample());
  ObjectContainerPtr examples = examplesStream->load()->randomize();
  StringDictionaryPtr labels = examples->getAndCast<VariableSetExample>(0)->getTargetVariables()->getVariablesDictionary();

  size_t numFolds = 7;

  VariableSetModelPtr model =
      //independantClassificationVariableSetModel(createMaxentClassifier(labels));
      //optimisticClassificationVariableSetModel(createMaxentClassifier(labels))
      //iterativeClassificationVariableSetModel(createMaxentClassifier(labels), createMaxentClassifier(labels));
      simulatedIterativeClassificationVariableSetModel(createMaxentClassifier(labels));

  std::cout << std::endl << std::endl << "FOLD " << targetFold << " / " << numFolds << " ..." << std::endl;
    
  ObjectContainerPtr trainingData = examples->invFold(targetFold, numFolds);
  ObjectContainerPtr testingData = examples->fold(targetFold, numFolds);

  ReferenceCountedObjectPtr<TestTrainingProgressCallback> callback
    = new TestTrainingProgressCallback(createLearningStoppingCriterion(), testingData, *resultsFileStream);
  model->trainBatch(trainingData, callback);
  
  std::cout << "Train Accuracy = " << callback->trainAccuracy << std::endl;
  std::cout << "Test Accuracy = " << callback->testAccuracy << std::endl;
  
  delete resultsFileStream;
 
  // Results: Prediction of SS3 / 7 folds CV
  
  // ---CO---
  //
  // AA(7)+PSSM(7) with 1/1 train iter => 71.30
  // AA(7)+PSSM(7) with 2/2 train iter => 72.01
  // AA(7)+PSSM(7) with 2/10 train iter => 72.29
 
  
  
  // ---ICA---
  // AA(7)+PSSM(7) + PR(1) with 2/2 train iter => 70.96
  // AA(7)+PSSM(7) + PR(2) with 2/2 train iter => 71.67
  // AA(7)+PSSM(7) + PR(5) with 2/2 train iter => 71.63
  
  // ---SICA---
  // AA(7)+PSSM(7) + PR(2) with 2/2 train iter, reg 0, maxpass 5 => 70.61
  // AA(7)+PSSM(7) + PR(2) with 2/10 train iter, reg 0.0001, maxpass 10
  
  // ---CO Results with only 1 folds over the seven, 10 training iteration, best test score---
  // AA(7)+PSSM(7) => 74.02
  // AA(7)+PSSM(7)+POS/LEN => 74.16
  // AA(7)+PSSM(7)+POS/LEN/CONJ(AA) => 74.19
  // AA(7)+PSSM(7)+POS%/LEN/CONJ(AA) => 74.07
  // AA(7)+PSSM(7)+POS/LEN/CONJ(AA)/CONJ(PSSM) => 74.00

  // ---Test Results on the whole dataset (Predition of SS3)---
  // AA(15) => 62.2
  // PSSM(15) => 72.1
  // AA(15)+PSSM(15) => 73.9
  // AA(15)+PSSM(15)+OPT(1) => 88.3
  // AA(15)+PSSM(15)+OPT(10) => 89.1
  // AA(15)+PSSM(15)+OPT_8(10) => 90.5
  // PSSM(15)+OPT(1) => 87.8
  // AA(15)+OPT(1) => 87.6
  // OPT(1) => 82.8
  // OPT(10) => 84.7
  return 0;
}
