/*-----------------------------------------.---------------------------------.
| Filename: SandBox.lcpp                   | Test Learning                   |
| Author  : Francis Maes                   |                                 |
| Started : 08/04/2010 16:27               |                                 |
`------------------------------------------/                                 |
                               |                                             |
                               `--------------------------------------------*/

#include <lbcpp/lbcpp.h>
#include "InferenceData/ScoreSymmetricMatrix.h"
#include "InferenceData/CommonFeatureGenerators.h"
#include "Protein/Evaluation/ProteinEvaluationCallback.h"
#include "Protein/Evaluation/ProteinStatisticsCalculator.h"
#include "Protein/SecondaryStructureDictionary.h"
#include "Protein/Inference/Protein1DInferenceStep.h"
#include "Protein/Inference/ProteinBackboneBondSequenceInferenceStep.h"
#include "Protein/Inference/ProteinContactMapInferenceStep.h"
#include "Protein/Inference/ProteinTertiaryStructureInferenceStep.h"
#include "Protein/Inference/ProteinInference.h"
using namespace lbcpp;

extern void declareProteinClasses();


class LocalRandomizedScalarLinearInferenceLearningCallback : public ScalarInferenceLearningCallback
{
public:
  LocalRandomizedScalarLinearInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, ScalarVectorFunctionPtr regularizer = ScalarVectorFunctionPtr(), bool normalizeLearningRate = true)
    : ScalarInferenceLearningCallback(step), learningRate(learningRate), regularizer(regularizer), normalizeLearningRate(normalizeLearningRate) {}

  virtual size_t learningEpoch(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr exampleLoss)
  {
    updateInputSize(features);

    if (exampleLoss->compute(1.0) > exampleLoss->compute(-1.0) && RandomGenerator::getInstance().sampleBool(0.8))
      return 0; // reject 80% of negative examples

    enum {randomizationSize = 10000};

    examples.push_back(std::make_pair(features, exampleLoss));
    if (examples.size() < 10000)
      return 0;

    std::vector<size_t> order;
    RandomGenerator::getInstance().sampleOrder(examples.size(), order);
    for (size_t i = 0; i < order.size(); ++i)
      processExample(examples[order[i]].first, examples[order[i]].second);
    applyRegularizer(step->getParameters(), regularizer, computeAlpha() * (double)examples.size());
    examples.clear();
    return order.size();
  }

  void processExample(FeatureGeneratorPtr features, ScalarFunctionPtr exampleLoss)
  {
    DenseVectorPtr parameters = step->getParameters();
    double alpha = computeAlpha();
    double prediction = features->dotProduct(parameters);
    features->addWeightedTo(parameters, - alpha * exampleLoss->computeDerivative(prediction));
    step->validateParametersChange();
  }

  void applyRegularizer(DenseVectorPtr parameters, ScalarVectorFunctionPtr regularizer, double alpha)
    {regularizer->computeGradient(parameters)->addWeightedTo(parameters, - alpha);}

  virtual void finishInferencesCallback()
  {
    std::cout << "Epoch " << epoch << ", " << step->getParameters()->l0norm() << " parameters, L2 = " << step->getParameters()->l2norm() << std::endl;
  }

protected:
  IterationFunctionPtr learningRate;
  ScalarVectorFunctionPtr regularizer;
  bool normalizeLearningRate;

  std::vector< std::pair<FeatureGeneratorPtr, ScalarFunctionPtr> > examples;

  double computeAlpha() const
  {
    double res = 1.0;
    if (learningRate)
      res *= learningRate->compute(epoch);
    if (normalizeLearningRate && inputSize.getMean())
      res /= inputSize.getMean();
    return res;
  }
};

class MyScalarLinearInferenceLearningCallback : public ScalarInferenceLearningCallback
{
public:
  MyScalarLinearInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, ScalarVectorFunctionPtr regularizer = ScalarVectorFunctionPtr(), bool normalizeLearningRate = true)
    : ScalarInferenceLearningCallback(step), learningRate(learningRate), regularizer(regularizer), normalizeLearningRate(normalizeLearningRate) {}

  virtual size_t learningEpoch(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr exampleLoss)
  {
    updateInputSize(features);
    if (exampleLoss->compute(1.0) > exampleLoss->compute(-1.0) && RandomGenerator::getInstance().sampleBool(0.8))
      return 0; // reject 80% of negative examples

    examples.push_back(std::make_pair(features, exampleLoss));
    return 0;
  }

  virtual void postInferenceCallback(InferenceStackPtr stack, ObjectPtr input, ObjectPtr supervision, ObjectPtr& output, ReturnCode& returnCode)
  {
    if (stack->getCurrentInference() == step && supervision && output)
    {
      currentParentStep = stack->getParentInference();
      FeatureGeneratorPtr features = input.dynamicCast<FeatureGenerator>();
      ScalarFunctionPtr loss = supervision.dynamicCast<ScalarFunction>();
      ScalarPtr prediction = output.dynamicCast<Scalar>();
      jassert(features && loss && prediction);
      epoch += learningEpoch(epoch, features, prediction->getValue(), loss);
    }
    else if (stack->getCurrentInference() == currentParentStep)
      epoch += flushExamples();
  }

  InferencePtr currentParentStep;

  size_t flushExamples()
  {
    std::vector<size_t> order;
    RandomGenerator::getInstance().sampleOrder(examples.size(), order);
    for (size_t i = 0; i < order.size(); ++i)
      processExample(examples[order[i]].first, examples[order[i]].second);
    applyRegularizer(step->getParameters(), regularizer, computeAlpha() * (double)examples.size());
    examples.clear();
    return order.size();
  }

  void processExample(FeatureGeneratorPtr features, ScalarFunctionPtr exampleLoss)
  {
    DenseVectorPtr parameters = step->getParameters();
    double alpha = computeAlpha();
    double prediction = features->dotProduct(parameters);
    features->addWeightedTo(parameters, - alpha * exampleLoss->computeDerivative(prediction));
    step->validateParametersChange();
  }

  void applyRegularizer(DenseVectorPtr parameters, ScalarVectorFunctionPtr regularizer, double alpha)
    {if (regularizer) regularizer->computeGradient(parameters)->addWeightedTo(parameters, - alpha);}

  virtual void finishInferencesCallback()
  {
    std::cout << "Epoch " << epoch << ", " << step->getParameters()->l0norm() << " parameters, L2 = " << step->getParameters()->l2norm() << std::endl;
  }

protected:
  IterationFunctionPtr learningRate;
  ScalarVectorFunctionPtr regularizer;
  bool normalizeLearningRate;

  std::vector< std::pair<FeatureGeneratorPtr, ScalarFunctionPtr> > examples;

  double computeAlpha() const
  {
    double res = 1.0;
    if (learningRate)
      res *= learningRate->compute(epoch);
    if (normalizeLearningRate && inputSize.getMean())
      res /= inputSize.getMean();
    return res;
  }
};

class MyInferenceLearnerCallback : public InferenceLearnerCallback
{
public:
  MyInferenceLearnerCallback(ObjectContainerPtr trainingData, ObjectContainerPtr testingData, bool useCacheOnTestingData = true)
    : trainingData(trainingData), testingData(testingData), startingTime(Time::getMillisecondCounter())
  {
    if (useCacheOnTestingData)
      cache = new InferenceResultCache();
  }

  virtual InferenceContextPtr createContext()
    {return singleThreadedInferenceContext();}
  
  virtual InferenceCallbackPtr createLearningCallback(LearnableAtomicInferencePtr step, InferencePtr parentStep)
  {
    if (parentStep.dynamicCast<ProteinContactMapInferenceStep>())
    {
      LearnableAtomicInferencePtr linearInference = step.dynamicCast<LearnableAtomicInference>();
      jassert(linearInference);
      double l2regularizer = 0;//.0001;
      return new MyScalarLinearInferenceLearningCallback(linearInference,
          invLinearIterationFunction(2, 250000),
          l2regularizer ? sumOfSquaresFunction(l2regularizer) : ScalarVectorFunctionPtr());
    }
    return InferenceCallbackPtr();
  }
  
  virtual double getProbabilityToCreateAnExample(InferenceStackPtr stack, ObjectPtr input, ObjectPtr supervision)
  {
    String inferenceStepName = stack->getInference(1)->getName();
    if (inferenceStepName == T("Init Step"))
      return 0.0;
    if (inferenceStepName == T("CA Update Pass"))
    {
      //RefineCAlphaPositionsInferenceStepPtr step = stack->getParentInference().dynamicCast<RefineCAlphaPositionsInferenceStep>();
      return 0.01;
    }

    if (inferenceStepName.startsWith(T("RR")))
    {
      ScalarFunctionPtr loss = supervision.dynamicCast<ScalarFunction>();
      jassert(loss);
      return loss->compute(0.0) < loss->compute(1.0)
        ? 0.05 // 5% probability for negative residue-residue contact examples
        : 1.0; // 100% probability for positive contacts
    }

    if (inferenceStepName.startsWith(T("DR")))
    {
      LabelPtr label = supervision.dynamicCast<Label>();
      jassert(label);
      return label->getIndex() == 1 ? 1.0 : 0.2; // 20% probability for negative disorder examples
    }
    return 1.0;
  }

  virtual ClassifierPtr createClassifier(InferenceStackPtr stack, FeatureDictionaryPtr labels)
  {
    std::cout << "CreateClassifier for step " << stack->getInference(1)->getName() << std::endl;
    if (labels == BinaryClassificationDictionary::getInstance())
    {
      IterationFunctionPtr learningRate = invLinearIterationFunction(0.5, 250000);
      GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
      GradientBasedBinaryClassifierPtr classifier = linearSVMBinaryClassifier(learner, labels);
      classifier->setL2Regularizer(0.01);
      return classifier;
    }
    else
    {
      IterationFunctionPtr learningRate = invLinearIterationFunction(2.0, 250000);
      GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
      return maximumEntropyClassifier(learner, labels, 20.0);
    }
  }

  virtual RegressorPtr createRegressor(InferenceStackPtr stack)
  {
    String inferenceStepName = stack->getInference(1)->getName();
      
    std::cout << "CreateRegressor for step " << inferenceStepName << std::endl;
    static const double regularizer = 0.0;

    IterationFunctionPtr learningRate;
    if (inferenceStepName.startsWith(T("BBB")))
      learningRate = constantIterationFunction(0.1);
    else if (inferenceStepName.startsWith(T("CA")))
      learningRate = constantIterationFunction(0.5);
    else if (inferenceStepName.startsWith(T("TS")))
      learningRate = constantIterationFunction(0.0001);
    else if (inferenceStepName.startsWith(T("RR")))
      learningRate = invLinearIterationFunction(0.5, 250000);
    else
      learningRate = invLinearIterationFunction(0.5, 150000);

    GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
    return generalizedLinearRegressor(learner, regularizer);
  }

  virtual void preLearningIterationCallback(size_t iterationNumber)
    {std::cout << std::endl << " ================== ITERATION " << iterationNumber << " ================== " << (Time::getMillisecondCounter() - startingTime) / 1000.0 << " s" <<  std::endl;}

  // returns false if learning should stop
  virtual bool postLearningIterationCallback(InferencePtr inference, size_t iterationNumber)
  {
    InferenceContextPtr validationContext = createContext();
    ProteinEvaluationCallbackPtr evaluation = new ProteinEvaluationCallback();
    validationContext->appendCallback(evaluation);
    if (cache)
      validationContext->appendCallback(cacheInferenceCallback(cache, inference));

    juce::uint32 startTime = Time::getMillisecondCounter();
    validationContext->runWithSupervisedExamples(inference, trainingData);
    double length = (Time::getMillisecondCounter() - startTime) / 1000.0;
    std::cout << "Train evaluation time: " << length << "s" << std::endl;
    std::cout << "Train evaluation: " << evaluation->toString() << std::endl;
    //double trainRmse = 
    
    validationContext->runWithSupervisedExamples(inference, testingData);
    std::cout << "Test evaluation: " << evaluation->toString() << std::endl;

   /* static double bestTestAcc = 0.0;
    double testAcc = evaluation->getDefaultScoreForTarget(T("SecondaryStructureSequence"));
    if (testAcc > bestTestAcc)
      bestTestAcc = testAcc;
    std::cout << "Best Test Accuracy: " << String(bestTestAcc * 100, 2) << "%" << std::endl;
*/
    /*double testRmse = evaluation->getPSSMRootMeanSquareError();

    if (trainRmse < bestTrainRmse)
    {
      bestTrainRmse = trainRmse;
      bestTestRmse = testRmse;
    }*/

    // stopping criterion
    return iterationNumber < 150;
  }

  virtual void preLearningStepCallback(InferencePtr step)
  {
    String passName = step->getName();
    std::cout << std::endl << "=====================================================" << std::endl;
    std::cout << "======= LEARNING PASS " << passName << " ==========" << (Time::getMillisecondCounter() - startingTime) / 1000 << " s" << std::endl;
    std::cout << "=====================================================" << std::endl;
    //bestTrainRmse = bestTestRmse = DBL_MAX;
  }

  virtual void postLearningStepCallback(InferencePtr step)
  {
    //std::cout << "Best Train RMSE: " << bestTrainRmse << std::endl;
    //std::cout << "Best Test RMSE: " << bestTestRmse << std::endl;
  }

private:
  ObjectContainerPtr trainingData;
  ObjectContainerPtr testingData;
  InferenceResultCachePtr cache;
  juce::uint32 startingTime;

//  double bestTrainRmse, bestTestRmse;
};

ObjectContainerPtr loadProteins(const File& directory, size_t maxCount = 0)
{
  ObjectStreamPtr proteinsStream = directoryObjectStream(directory, T("*.protein"));
#ifdef JUCE_DEBUG
  ObjectContainerPtr res = proteinsStream->load(maxCount ? maxCount : 7)->randomize();
#else
  ObjectContainerPtr res = proteinsStream->load(maxCount)->randomize();
#endif
  for (size_t i = 0; i < res->size(); ++i)
  {
    ProteinPtr protein = res->getAndCast<Protein>(i);
    jassert(protein);
    protein->computeMissingFields();
  }
  return res;
}

int main(int argc, char** argv)
{
  declareProteinClasses();

  File modelDirectory(T("C:\\Projets\\LBC++\\projects\\temp\\Models\\RR.model"));

  File smallPDBDirectory(T("C:\\Projets\\LBC++\\projects\\temp\\SmallPDB\\protein"));
  ObjectContainerPtr smallPDBProteins = loadProteins(smallPDBDirectory, 7);
  std::cout << "SmallPDB " << ProteinStatisticsCalculator::computeStatistics(smallPDBProteins) << std::endl;
  smallPDBProteins = smallPDBProteins->apply(new ObjectToObjectPairFunction());

  //proteins = proteins->apply(new ProteinToInputOutputPair());

  ObjectContainerPtr trainingData = smallPDBProteins->size() >= 7 ? smallPDBProteins->invFold(0,7) : smallPDBProteins;
  ObjectContainerPtr testingData = smallPDBProteins->size() >= 7 ? smallPDBProteins->fold(0,7) : smallPDBProteins;

  std::cout << trainingData->size() << " Training Proteins "
            << testingData->size() << " Testing Proteins" << std::endl;


  /*
  ** Creation of the feature function
  */
  CompositeProteinResidueFeaturesPtr featureFunction = new CompositeProteinResidueFeatures();

  featureFunction->addSubFeatures(proteinUnitResidueFeature());
  //featureFunction->addSubFeatures(proteinPositionIndexResidueFeature()); // DEBUG !!

  //featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("AminoAcidSequence"), 8, 8, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("PositionSpecificScoringMatrix"), 8, 8, true));
  /*featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("SecondaryStructureSequence"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("DSSPSecondaryStructureSequence"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("SolventAccessibilityThreshold20"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("DisorderProbabilitySequence"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("BackboneBondSequence"), 5, 5, true));*/

  // New features :
  //featureFunction->addSubFeatures(proteinPositionFeatures());
  //featureFunction->addSubFeatures(proteinLengthFeatures());
  //featureFunction->addSubFeatures(proteinSegmentConjunctionFeatures(T("SecondaryStructureSequence"), 5));

  CompositeProteinResiduePairFeaturesPtr pairFeatureFunction = new CompositeProteinResiduePairFeatures();
  pairFeatureFunction->addSubFeatures(proteinPointResiduePairFeatures(featureFunction));
  //pairFeatureFunction->addSubFeatures(separationLengthResiduePairFeatures());
  //pairFeatureFunction->addSubFeatures(new TriDimProteinResiduePairFeatures());
  //pairFeatureFunction->addSubFeatures(proteinPositionIndexResiduePairFeature());

  /*
  ** Creation of the inference 
  */
  ProteinInferencePtr proteinInference = new ProteinInference();
  proteinInference->setPDBDebugDirectory(File(T("C:\\Projets\\LBC++\\projects\\temp\\pdbs")));

  for (size_t i = 0; i < 1; ++i)
  {
    Protein2DInferenceStepPtr step = new ProteinContactMapInferenceStep(T("RR Pass ") + lbcpp::toString(i),
                                                       pairFeatureFunction, T("ResidueResidueContactMatrix8Ca"));
    proteinInference->appendStep(step);
    
#if 0
      step = new ProteinSequenceLabelingInferenceStep(T("SS3 Pass ") + lbcpp::toString(i), featureFunction, /*T("SecondaryStructureProbabilities"), */T("SecondaryStructureSequence"));
      proteinInference->appendStep(step);
      step = new ProteinSequenceLabelingInferenceStep(T("DR Pass ") + lbcpp::toString(i), featureFunction, T("DisorderProbabilitySequence"), T("DisorderSequence"));
      proteinInference->appendStep(step);
      step = new ProteinSequenceLabelingInferenceStep(T("SA Pass ") + lbcpp::toString(i), featureFunction, T("SolventAccessibilityThreshold20"));
      proteinInference->appendStep(step);
      step = new ProteinBackboneBondSequenceInferenceStep(T("BBB Pass ") + lbcpp::toString(i), featureFunction);
      proteinInference->appendStep(step);
#endif // 0
  }
  //std::cout << "Inference: " << proteinInference->toString() << std::endl;

  /*
  ** Learning
  */
  InferenceLearnerCallbackPtr callback = new MyInferenceLearnerCallback(trainingData, testingData, false);
  //InferenceLearnerPtr learner = stepByStepDeterministicSimulationLearner(callback, true);//, modelDirectory, true);
  InferenceLearnerPtr learner = globalSimulationLearner(callback);
  learner->train(proteinInference, trainingData);

  /*
  ** Evaluation
  */
  InferenceContextPtr validationContext = singleThreadedInferenceContext();
  ProteinEvaluationCallbackPtr evaluation = new ProteinEvaluationCallback();
  validationContext->appendCallback(evaluation);

  validationContext->runWithSupervisedExamples(proteinInference, trainingData);
  std::cout << "Train evaluation: " << evaluation->toString() << std::endl;

  validationContext->runWithSupervisedExamples(proteinInference, testingData);
  std::cout << "Test evaluation: " << evaluation->toString() << std::endl;

  return 0;
}
