/*-----------------------------------------.---------------------------------.
| Filename: SandBox.lcpp                   | Test Learning                   |
| Author  : Francis Maes                   |                                 |
| Started : 08/04/2010 16:27               |                                 |
`------------------------------------------/                                 |
                               |                                             |
                               `--------------------------------------------*/

#include <lbcpp/lbcpp.h>
#include "InferenceData/ScoreSymmetricMatrix.h"
#include "InferenceData/CommonFeatureGenerators.h"
#include "Protein/Evaluation/ProteinEvaluationCallback.h"
#include "Protein/Evaluation/ProteinStatisticsCalculator.h"
#include "Protein/SecondaryStructureDictionary.h"
#include "Protein/Inference/Protein1DInferenceStep.h"
#include "Protein/Inference/ProteinBackboneBondSequenceInferenceStep.h"
#include "Protein/Inference/ProteinContactMapInferenceStep.h"
#include "Protein/Inference/ProteinTertiaryStructureInferenceStep.h"
#include "Protein/Inference/ProteinInference.h"
#include <fstream>
using namespace lbcpp;

extern void declareProteinClasses();

class ScalarInferenceLearningCallback : public InferenceCallback
{
public:
  ScalarInferenceLearningCallback(LearnableAtomicInferencePtr step);

  virtual size_t postInferenceCallback(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr loss) = 0;
  virtual size_t postEpisodeCallback() {return 0;}

  virtual void postInferenceCallback(InferenceStackPtr stack, ObjectPtr input, ObjectPtr supervision, ObjectPtr& output, ReturnCode& returnCode);

protected:
  LearnableAtomicInferencePtr step;
  size_t epoch;
  ScalarVariableMean inputSize;
  InferencePtr currentParentStep;

  void updateInputSize(FeatureGeneratorPtr inputfeatures);
};

/*
** ScalarInferenceLearningCallback
*/
ScalarInferenceLearningCallback::ScalarInferenceLearningCallback(LearnableAtomicInferencePtr step)
    : step(step), epoch(1) {}

void ScalarInferenceLearningCallback::postInferenceCallback(InferenceStackPtr stack, ObjectPtr input, ObjectPtr supervision, ObjectPtr& output, ReturnCode& returnCode)
{
  if (step == stack->getCurrentInference() && supervision && output)
  {
    currentParentStep = stack->getParentInference();
    FeatureGeneratorPtr features = input.dynamicCast<FeatureGenerator>();
    ScalarFunctionPtr loss = supervision.dynamicCast<ScalarFunction>();
    ScalarPtr prediction = output.dynamicCast<Scalar>();
    jassert(features && loss && prediction);
    epoch += postInferenceCallback(epoch, features, prediction->getValue(), loss);
  }
  else if (stack->getCurrentInference() == currentParentStep)
    epoch += postEpisodeCallback();
}

void ScalarInferenceLearningCallback::updateInputSize(FeatureGeneratorPtr inputfeatures)
{
  // computing the l1norm() may be long, so we make more and more sparse sampling of this quantity
  if (inputSize.getCount() < 10 ||                         // every time until having 10 samples
      (inputSize.getCount() < 100 && (epoch % 10 == 0)) || // every 10 epochs until having 100 samples
      (epoch % 100 == 0))                                  // every 100 epochs after that
    inputSize.push((double)(inputfeatures->l1norm()));
}

// -

class IterativeScalarInferenceLearningCallback : public ScalarInferenceLearningCallback
{
public:
  IterativeScalarInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, bool normalizeLearningRate = true)
    : ScalarInferenceLearningCallback(step), lossValue(T("Loss")), learningRate(learningRate), normalizeLearningRate(normalizeLearningRate) {}

  virtual void finishInferencesCallback()
  {
    std::cout << step->getName() << " Epoch " << epoch << ", " << step->getParameters()->l0norm() << " parameters, L2 = " << String(step->getParameters()->l2norm(), 3) << std::endl;
    if (lossValue.getCount())
    {
      std::cout << lossValue.toString() << std::endl;
      lossValue.clear();
    }
    std::cout << std::endl;
  }

protected:
  ScalarVariableMean lossValue;
  IterationFunctionPtr learningRate;
  bool normalizeLearningRate;

  double computeAlpha() const
  {
    double res = 1.0;
    if (learningRate)
      res *= learningRate->compute(epoch);
    if (normalizeLearningRate && inputSize.getMean())
      res /= inputSize.getMean();
    return res;
  }

  void applyRegularizer(DenseVectorPtr parameters, ScalarVectorFunctionPtr regularizer, double alpha)
  {
    if (regularizer)
    {
      regularizer->computeGradient(parameters)->addWeightedTo(parameters, - alpha);
      step->validateParametersChange();
    }
  }
};
/*
class MyClassificationExamplesPrinter : public LearningDataObjectPrinter
{
public:
  MyClassificationExamplesPrinter(const File& file)
    : LearningDataObjectPrinter(file), featureMapping(new StringDictionary()) {featureMapping->add(String::empty);}
  
  virtual void consume(ObjectPtr object)
  {
    ObjectPairPtr example = object.dynamicCast<ObjectPair>();
    jassert(example);
    FeatureGeneratorPtr features = example->getFirst().dynamicCast<FeatureGenerator>();
    ScalarFunctionPtr loss = example->getSecond().dynamicCast<ScalarFunction>();
    if (features && loss)
    {
      bool isNegativeExample = loss->compute(1.0) > loss->compute(-1.0);
      print(isNegativeExample ? T("-1 ") : T("+1 "));
      printFeatureList(features, featureMapping);
      printNewLine();
    }
  }
  
private:
  StringDictionaryPtr featureMapping;
};

class ConsumeInferenceCallback : public InferenceCallback
{
public:
  ConsumeInferenceCallback(InferencePtr inference, ObjectConsumerPtr consumer)
    : inference(inference), consumer(consumer) {}
  
  virtual void preInferenceCallback(InferenceStackPtr stack, ObjectPtr& input, ObjectPtr& supervision, ObjectPtr& output, ReturnCode& returnCode)
  {
    if (stack->getCurrentInference() == inference)
      consumer->consume(new ObjectPair(input, supervision));
  }

protected:
  InferencePtr inference;
  ObjectConsumerPtr consumer;
};
*/

class SimpleBipartiteRankingScalarInferenceLearningCallback : public IterativeScalarInferenceLearningCallback
{
public:
  SimpleBipartiteRankingScalarInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, ScalarVectorFunctionPtr regularizer = ScalarVectorFunctionPtr(), bool normalizeLearningRate = true)
    : IterativeScalarInferenceLearningCallback(step, learningRate, normalizeLearningRate), regularizer(regularizer) {}

  virtual size_t postInferenceCallback(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr exampleLoss)
  {
    bool isNegativeExample = exampleLoss->compute(1.0) > exampleLoss->compute(-1.0);
    if (isNegativeExample)
      negativeExamples.push_back(std::make_pair(features, prediction));
    else
      positiveExamples.push_back(std::make_pair(features, prediction));
    return 0;
  }

  virtual size_t postEpisodeCallback()
  {
    size_t numExamples = positiveExamples.size() < negativeExamples.size() ? positiveExamples.size() : negativeExamples.size();
    for (size_t j = 0; j < 1; ++j)
    {
      std::vector<size_t> order1, order2;
      RandomGenerator::getInstance().sampleOrder(positiveExamples.size(), order1);
      RandomGenerator::getInstance().sampleOrder(negativeExamples.size(), order2);
      double k = computeAlpha();
      for (size_t i = 0; i < numExamples; ++i)
      {
        FeatureGeneratorPtr positive = positiveExamples[order1[i]].first;
        FeatureGeneratorPtr negative = negativeExamples[order2[i]].first;
        DenseVectorPtr parameters = step->getParameters();
        double positiveScore = parameters->dotProduct(positive);
        double negativeScore = parameters->dotProduct(negative);
        double deltaScore = positiveScore - negativeScore;
        double positiveWeight = 0.0;
        double negativeWeight = 0.0;

        if (deltaScore < 1)
          positiveWeight = k, negativeWeight = -k;

        static const double regressionWeight = 0.0;
        if (positiveScore < 1)
          positiveWeight += k * regressionWeight;// * (1 - positiveScore);
        if (negativeScore > -1)
          negativeWeight += k * regressionWeight * (-1);// - negativeScore);

        if (positiveWeight)
          step->getParameters()->addWeighted(positive, positiveWeight);
        if (negativeWeight)
          step->getParameters()->addWeighted(negative, negativeWeight);
      }
      applyRegularizer(step->getParameters(), regularizer, computeAlpha() * (double)numExamples);

      // tune bias
      ROCAnalyse analyse;
      for (size_t i = 0; i < positiveExamples.size(); ++i)
        analyse.addPrediction(step->getParameters()->dotProduct(positiveExamples[i].first), true);
      for (size_t i = 0; i < negativeExamples.size(); ++i)
        analyse.addPrediction(step->getParameters()->dotProduct(negativeExamples[i].first), false);

      double bestF1Score;
      double bestThreshold = analyse.findBestThreshold(bestF1Score);
      std::cout << "BEST Threshold: " << bestThreshold << " => F1 = " << String(bestF1Score * 100, 2) << "%" << std::endl;
      /*DenseVectorPtr unitParams = step->getParameters()->getSubVector(T("unit"));
      if (unitParams)
        unitParams->get(0) -= 0.1 * bestThreshold;*/
    }
 
    positiveExamples.clear();
    negativeExamples.clear();
    return numExamples * 1;
  }

protected:
  ScalarVectorFunctionPtr regularizer;
  std::vector< std::pair<FeatureGeneratorPtr, double> > positiveExamples;
  std::vector< std::pair<FeatureGeneratorPtr, double> > negativeExamples;
};

class IterativeDelayedScalarInferenceLearningCallback : public IterativeScalarInferenceLearningCallback
{
public:
  IterativeDelayedScalarInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, bool normalizeLearningRate = true)
    : IterativeScalarInferenceLearningCallback(step, learningRate, normalizeLearningRate) {}

protected:
  std::vector< std::pair<FeatureGeneratorPtr, ScalarFunctionPtr> > examples;

  virtual void processExample(FeatureGeneratorPtr features, ScalarFunctionPtr exampleLoss)
  {
    DenseVectorPtr parameters = step->getParameters();
    double alpha = computeAlpha();
    double prediction = features->dotProduct(parameters);
    lossValue.push(exampleLoss->compute(prediction));
    features->addWeightedTo(parameters, - alpha * exampleLoss->computeDerivative(prediction));
    step->validateParametersChange();
  }
};

class LocalRandomizedScalarLinearInferenceLearningCallback : public IterativeDelayedScalarInferenceLearningCallback
{
public:
  LocalRandomizedScalarLinearInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, ScalarVectorFunctionPtr regularizer = ScalarVectorFunctionPtr(), bool normalizeLearningRate = true)
    : IterativeDelayedScalarInferenceLearningCallback(step, learningRate, normalizeLearningRate), regularizer(regularizer), numPositives(0), numNegatives(0) {}

  virtual size_t postInferenceCallback(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr exampleLoss)
  {
    bool isNegativeExample = exampleLoss->compute(1.0) > exampleLoss->compute(-1.0);
    if (isNegativeExample && RandomGenerator::getInstance().sampleBool(0.98))
      return 0; //reject 98% of negative examples

    isNegativeExample ? ++numNegatives : ++numPositives;
    updateInputSize(features);

    examples.push_back(std::make_pair(features, exampleLoss));
    //if (examples.size() == 10000)
      flushExamples();
    return 1;
  }

  size_t flushExamples()
  {
    //std::cout << "Flush " << examples.size() << " examples (" << numPositives << " positives, " << numNegatives << " negatives)..." << std::flush;
    std::vector<size_t> order;
    RandomGenerator::getInstance().sampleOrder(examples.size(), order);
    for (size_t i = 0; i < order.size(); ++i)
      processExample(examples[order[i]].first, examples[order[i]].second);
    applyRegularizer(step->getParameters(), regularizer, computeAlpha() * (double)examples.size());
    examples.clear();
    numPositives = numNegatives = 0;
    //std::cout << "ok." << std::endl;
    return order.size();
  }

  virtual void finishInferencesCallback()
  {
    flushExamples();
    IterativeScalarInferenceLearningCallback::finishInferencesCallback();
  }

protected:
  ScalarVectorFunctionPtr regularizer;
  size_t numPositives, numNegatives;
};

#include <deque>
class ScalarVariableRecentMean : public NameableObject
{
public:
  ScalarVariableRecentMean(const String& name = T("unnamed"), size_t historySize = 10000)
    : NameableObject(name), historySize(historySize), sum(0.0), epoch(0) {}

  void push(double value)
  {
    values.push_back(value);
    sum += value;
    if (values.size() > historySize)
    {
      sum -= values.front();
      values.pop_front();
    }
    ++epoch;
    if (epoch % historySize == 0)
      sum = computeSum();
  }

  double getSum() const
    {return sum;}

  size_t getCount() const
    {return values.size();}

private:
  std::deque<double> values;
  size_t historySize;
  double sum;

  size_t epoch;

  double computeSum() const
  {
    double res = 0.0;
    for (std::deque<double>::const_iterator it = values.begin(); it != values.end(); ++it)
      res += *it;
    return res;
  }
};

class PerEpisodeRandomizedLinearInferenceLearningCallback : public IterativeDelayedScalarInferenceLearningCallback
{
public:
  PerEpisodeRandomizedLinearInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, ScalarVectorFunctionPtr regularizer = ScalarVectorFunctionPtr(), bool normalizeLearningRate = true)
    : IterativeDelayedScalarInferenceLearningCallback(step, learningRate, normalizeLearningRate), regularizer(regularizer) {}

  virtual size_t postInferenceCallback(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr exampleLoss)
  {
    //if (RandomGenerator::getInstance().sampleBool(0.9))
    //  return 0; //reject 90% of examples
    bool isNegativeExample = exampleLoss->compute(1.0) > exampleLoss->compute(-1.0);
    if (isNegativeExample && RandomGenerator::getInstance().sampleBool(0.8))
      return 0; // reject 80% of negative examples
 
    updateInputSize(features);
    examples.push_back(std::make_pair(features, exampleLoss));
    return 0;
  }

  virtual size_t postEpisodeCallback()
  {
    std::vector<size_t> order;
    RandomGenerator::getInstance().sampleOrder(examples.size(), order);
    for (size_t i = 0; i < order.size(); ++i)
      processExample(examples[order[i]].first, examples[order[i]].second);
    applyRegularizer(step->getParameters(), regularizer, computeAlpha() * (double)examples.size());
    examples.clear();
    return order.size();
  }

  virtual void processExample(FeatureGeneratorPtr features, ScalarFunctionPtr exampleLoss)
  {
    DenseVectorPtr parameters = step->getParameters();
    double alpha = computeAlpha();
    double prediction = features->dotProduct(parameters);

    bool isNegativeExample = exampleLoss->compute(1.0) > exampleLoss->compute(-1.0);
    truePositives.push(prediction > 0 && !isNegativeExample ? 1.0 : 0.0);
    falsePositives.push(prediction > 0 && isNegativeExample ? 1.0 : 0.0);
    falseNegatives.push(prediction < 0 && !isNegativeExample ? 1.0 : 0.0);
    trueNegatives.push(prediction < 0 && isNegativeExample ? 1.0 : 0.0);
  
    /*double f1derivativeWRTtp = 0.001;
    double f1derivativeWRTtn = 0.001;
    double f1 = 0.0;
    double unionSize = 2.0 * truePositives.getSum() + falsePositives.getSum() + falseNegatives.getSum();
    if (unionSize)
    {
      f1 = 2.0 * truePositives.getSum() / unionSize;
      double invK = 2.0 / (unionSize * unionSize);
      f1derivativeWRTtp = juce::jlimit(0.0, 0.1, (truePositives.getSum() + falsePositives.getSum() + falseNegatives.getSum()) * invK);
      f1derivativeWRTtn = juce::jlimit(0.0, 0.1, truePositives.getSum() * invK);
      if (!f1derivativeWRTtp && !f1derivativeWRTtn)
        f1derivativeWRTtp = f1derivativeWRTtn = 0.001;
    }

    static std::ofstream ostr("C:\\Projets\\LBC++\\projects\\temp\\f1der.txt");
    static int epoch = 0;
    ostr << ++epoch << " " << f1derivativeWRTtp << " " << f1derivativeWRTtn << " " << f1 << std::endl;

    double importance = isNegativeExample ? f1derivativeWRTtn : f1derivativeWRTtp;*/
    double importance = 1.0;
    lossValue.push(importance * exampleLoss->compute(prediction));
    features->addWeightedTo(parameters, - alpha * importance * exampleLoss->computeDerivative(prediction));
    step->validateParametersChange();
  }

protected:
  ScalarVectorFunctionPtr regularizer;
  ScalarVariableRecentMean truePositives;
  ScalarVariableRecentMean falsePositives;
  ScalarVariableRecentMean falseNegatives;
  ScalarVariableRecentMean trueNegatives;
};

class PerEpisodeMiniBatchLinearInferenceLearningCallback : public IterativeScalarInferenceLearningCallback
{
public:
  PerEpisodeMiniBatchLinearInferenceLearningCallback(LearnableAtomicInferencePtr step, IterationFunctionPtr learningRate, ScalarVectorFunctionPtr regularizer = ScalarVectorFunctionPtr(), bool normalizeLearningRate = true)
    : IterativeScalarInferenceLearningCallback(step, learningRate, normalizeLearningRate), regularizer(regularizer), examplesCount(0), updateNumber(0) {}

  void addFeaturesToGradient(DenseVectorPtr& gradientAccumulator, FeatureGeneratorPtr features, double weight)
  {
    if (weight)
    {
      if (!gradientAccumulator)
        gradientAccumulator = new DenseVector(features->getDictionary());
      features->addWeightedTo(gradientAccumulator, weight);
    }
  }

  virtual size_t postInferenceCallback(size_t epoch, FeatureGeneratorPtr features, double prediction, ScalarFunctionPtr exampleLoss)
  {
    bool isNegativeExample = exampleLoss->compute(1.0) > exampleLoss->compute(-1.0);
    //if (isNegativeExample && RandomGenerator::getInstance().sampleBool(0.8))
    //  return 0; // reject 80% of negative examples

    examples.push_back(std::make_pair(features, !isNegativeExample));
    confusionMatrix.addPrediction(prediction > 0, !isNegativeExample);
    
    updateInputSize(features);
    DenseVectorPtr parameters = step->getParameters();
    
    if (isNegativeExample)
      addFeaturesToGradient(negativeGradientAccumulator, features, exampleLoss->computeDerivative(prediction));
    else
      addFeaturesToGradient(positiveGradientAccumulator, features, exampleLoss->computeDerivative(prediction));

    lossValue.push(exampleLoss->compute(prediction));
    ++examplesCount;

   // if (examplesCount == 100)
   //   flushGradient();

    return 1;
  }

  void flushGradient()
  {
    if (negativeGradientAccumulator || positiveGradientAccumulator)
    {
      /*double unionSize = (double)(confusionMatrix.getSampleCount() + confusionMatrix.getTruePositives() - confusionMatrix.getTrueNegatives());
      if (!unionSize)
        return;

      double f1 = 2.0 * confusionMatrix.getTruePositives() / unionSize;
      double invK = 2.0 / (unionSize * unionSize);
      double f1derivativeWRTtp = (confusionMatrix.getSampleCount() - confusionMatrix.getTrueNegatives()) * invK; 
      double f1derivativeWRTtn = confusionMatrix.getTruePositives() * invK; 

      //std::cout << "Confusion matrix: " << std::endl << confusionMatrix.toString();
      std::cout << "d(F1)/d(TP): " << f1derivativeWRTtp << " d(F1)/d(TN): " << f1derivativeWRTtn << std::endl; 
      confusionMatrix.clear();*/

      double k = - computeAlpha();
      if (updateNumber == 0)
        k /= 100.0;
      ++updateNumber;

      //std::cout << "Pos Gradient: " << (positiveGradientAccumulator ? positiveGradientAccumulator->l2norm() * k * f1derivativeWRTtp : 0.0)
      //  << " Neg Gradient: " << (negativeGradientAccumulator ? negativeGradientAccumulator->l2norm() * k * f1derivativeWRTtn : 0.0) << std::endl;
      if (negativeGradientAccumulator)// && f1derivativeWRTtn)
        negativeGradientAccumulator->addWeightedTo(step->getParameters(), k);// * juce::jmin(0.0001, f1derivativeWRTtn));

      if (positiveGradientAccumulator)// && f1derivativeWRTtp)
        positiveGradientAccumulator->addWeightedTo(step->getParameters(), k);// * juce::jmin(0.0001, f1derivativeWRTtp));

      applyRegularizer(step->getParameters(), regularizer, (double)examplesCount * computeAlpha());
      step->validateParametersChange();

      //std::cout << "Unit Pos gradient: " << pouetGetUnit(positiveGradientAccumulator) << " Neg: " << pouetGetUnit(negativeGradientAccumulator) << std::endl;
      //std::cout << "Unit parameter: " << pouetGetUnit(step->getParameters()) << " l2 = " << step->getParameters()->l2norm() << " loss = " << lossValue.getMean() << std::endl;
      ROCAnalyse analyse;
      for (size_t i = 0; i < examples.size(); ++i)
        analyse.addPrediction(step->getParameters()->dotProduct(examples[i].first), examples[i].second);
      double bestF1Score;
      double bestThreshold = analyse.findBestThreshold(bestF1Score);
      std::cout << "BEST Threshold: " << bestThreshold << " => F1 = " << String(bestF1Score * 100, 2) << "%" << std::endl;
      std::cout << step->getParameters()->getDictionary()->getScopes()->toString() << std::endl;
      DenseVectorPtr unitParams = step->getParameters()->getSubVector(T("unit"));
      std::cout << lbcpp::toString(unitParams) << std::endl;
      if (unitParams)
        unitParams->get(0) -= bestThreshold;

      examplesCount = 0;
      positiveGradientAccumulator = negativeGradientAccumulator = DenseVectorPtr();
    }
  }

  double pouetGetUnit(DenseVectorPtr vector)
  {
    if (!vector)
      return 0.0;
    vector = vector->getSubVector(T("MyResiduePairFeatures"));
    if (!vector)
      return 0.0;
    return vector->get(T("unit"));
  }

  virtual size_t postEpisodeCallback()
  {
    flushGradient();
    return 0;
  }

  virtual void finishInferencesCallback()
    {/*flushGradient();*/}

private:
  DenseVectorPtr positiveGradientAccumulator;
  DenseVectorPtr negativeGradientAccumulator;
  ScalarVectorFunctionPtr regularizer;
  size_t examplesCount;
  size_t updateNumber;

  std::vector<std::pair<FeatureGeneratorPtr, bool> > examples;

  BinaryClassificationConfusionMatrix confusionMatrix;
};

class MyInferenceLearnerCallback : public InferenceLearnerCallback
{
public:
  MyInferenceLearnerCallback(ObjectContainerPtr trainingData, ObjectContainerPtr testingData, bool useCacheOnTestingData = true)
    : trainingData(trainingData), testingData(testingData), startingTime(Time::getMillisecondCounter())
  {
    if (useCacheOnTestingData)
      cache = new InferenceResultCache();
  }

  virtual InferenceContextPtr createContext()
    {return singleThreadedInferenceContext();}

  virtual double getProbabilityToCreateAnExample(InferenceStackPtr stack, ObjectPtr input, ObjectPtr supervision)
  {
    String inferenceStepName = stack->getInference(1)->getName();
    if (inferenceStepName == T("Init Step"))
      return 0.0;
    if (inferenceStepName == T("CA Update Pass"))
    {
      //RefineCAlphaPositionsInferenceStepPtr step = stack->getParentInference().dynamicCast<RefineCAlphaPositionsInferenceStep>();
      return 0.01;
    }

    if (inferenceStepName.startsWith(T("RR")))
    {
      ScalarFunctionPtr loss = supervision.dynamicCast<ScalarFunction>();
      jassert(loss);
      return loss->compute(0.0) < loss->compute(1.0)
        ? 0.05 // 5% probability for negative residue-residue contact examples
        : 1.0; // 100% probability for positive contacts
    }

    if (inferenceStepName.startsWith(T("DR")))
    {
      LabelPtr label = supervision.dynamicCast<Label>();
      jassert(label);
      return label->getIndex() == 1 ? 1.0 : 0.2; // 20% probability for negative disorder examples
    }
    return 1.0;
  }

  virtual ClassifierPtr createClassifier(InferenceStackPtr stack, FeatureDictionaryPtr labels)
  {
    std::cout << "CreateClassifier for step " << stack->getInference(1)->getName() << std::endl;
    if (labels == BinaryClassificationDictionary::getInstance())
    {
      IterationFunctionPtr learningRate = invLinearIterationFunction(0.5, 250000);
      GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
      GradientBasedBinaryClassifierPtr classifier = linearSVMBinaryClassifier(learner, labels);
      classifier->setL2Regularizer(0.01);
      return classifier;
    }
    else
    {
      IterationFunctionPtr learningRate = invLinearIterationFunction(2.0, 250000);
      GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
      return maximumEntropyClassifier(learner, labels, 20.0);
    }
  }

  virtual RegressorPtr createRegressor(InferenceStackPtr stack)
  {
    String inferenceStepName = stack->getInference(1)->getName();
      
    std::cout << "CreateRegressor for step " << inferenceStepName << std::endl;
    static const double regularizer = 0.0;

    IterationFunctionPtr learningRate;
    if (inferenceStepName.startsWith(T("BBB")))
      learningRate = constantIterationFunction(0.1);
    else if (inferenceStepName.startsWith(T("CA")))
      learningRate = constantIterationFunction(0.5);
    else if (inferenceStepName.startsWith(T("TS")))
      learningRate = constantIterationFunction(0.0001);
    else if (inferenceStepName.startsWith(T("RR")))
      learningRate = invLinearIterationFunction(0.5, 250000);
    else
      learningRate = invLinearIterationFunction(0.5, 150000);

    GradientBasedLearnerPtr learner = stochasticDescentLearner(learningRate);  
    return generalizedLinearRegressor(learner, regularizer);
  }

  virtual void preLearningIterationCallback(size_t iterationNumber)
    {std::cout << std::endl << " ================== ITERATION " << iterationNumber << " ================== " << (Time::getMillisecondCounter() - startingTime) / 1000.0 << " s" <<  std::endl;}

  // returns false if learning should stop
  virtual bool postLearningIterationCallback(InferencePtr inference, size_t iterationNumber)
  {
    InferenceContextPtr validationContext = createContext();
    ProteinEvaluationCallbackPtr evaluation = new ProteinEvaluationCallback();
    validationContext->appendCallback(evaluation);
    if (cache)
      validationContext->appendCallback(cacheInferenceCallback(cache, inference));

    juce::uint32 startTime = Time::getMillisecondCounter();
    validationContext->runWithSupervisedExamples(inference, trainingData);
    double length = (Time::getMillisecondCounter() - startTime) / 1000.0;
    std::cout << "Train evaluation time: " << length << "s" << std::endl;
    std::cout << "Train evaluation: " << evaluation->toString() << std::endl;
    //double trainRmse = 

    static double bestTrainAcc = 0.0;
    double trainAcc = evaluation->getDefaultScoreForTarget(T("ResidueResidueContactMatrix8Ca"));
    if (trainAcc > bestTrainAcc)
      bestTrainAcc = trainAcc;
    std::cout << "Best Train F1: " << String(bestTrainAcc * 100.0, 2) << "%" << std::endl;


    validationContext->runWithSupervisedExamples(inference, testingData);
    std::cout << "Test evaluation: " << evaluation->toString() << std::endl;

    static double bestTestAcc = 0.0;
    double testAcc = evaluation->getDefaultScoreForTarget(T("ResidueResidueContactMatrix8Ca"));
    if (testAcc > bestTestAcc)
      bestTestAcc = testAcc;
    std::cout << "Best Test F1: " << String(bestTestAcc * 100.0, 2) << "%" << std::endl;

    static std::ofstream ostr("C:\\Projets\\LBC++\\projects\\temp\\Curve_stoch_98p_All_1_10e4.txt");
    ostr << iterationNumber << " " << trainAcc << " " << testAcc << std::endl;

    // stopping criterion
    return iterationNumber < 250;
  }
  
  virtual InferenceCallbackPtr createLearningCallback(LearnableAtomicInferencePtr step, InferencePtr parentStep)
  {
    if (parentStep.dynamicCast<ProteinContactMapInferenceStep>())
    {
      //return new ConsumeInferenceCallback(step, new MyClassificationExamplesPrinter(File(T("C:\\Projets\\LBC++\\projects\\temp\\twoprotRR.data"))));

      LearnableAtomicInferencePtr linearInference = step.dynamicCast<LearnableAtomicInference>();
      jassert(linearInference);
      double l2regularizer = 0;//.0001;
      return new LocalRandomizedScalarLinearInferenceLearningCallback(linearInference,
          invLinearIterationFunction(1.0, 10000),
          l2regularizer ? sumOfSquaresFunction(l2regularizer) : ScalarVectorFunctionPtr());
    }
    return InferenceCallbackPtr();
  }
  
  virtual void preLearningStepCallback(InferencePtr step)
  {
    String passName = step->getName();
    std::cout << std::endl << "=====================================================" << std::endl;
    std::cout << "======= LEARNING PASS " << passName << " ==========" << (Time::getMillisecondCounter() - startingTime) / 1000 << " s" << std::endl;
    std::cout << "=====================================================" << std::endl;
    //bestTrainRmse = bestTestRmse = DBL_MAX;
  }

  virtual void postLearningStepCallback(InferencePtr step)
  {
    //std::cout << "Best Train RMSE: " << bestTrainRmse << std::endl;
    //std::cout << "Best Test RMSE: " << bestTestRmse << std::endl;
  }

private:
  ObjectContainerPtr trainingData;
  ObjectContainerPtr testingData;
  InferenceResultCachePtr cache;
  juce::uint32 startingTime;

//  double bestTrainRmse, bestTestRmse;
};


ObjectContainerPtr loadProteins(const File& directory, size_t maxCount = 0)
{
  ObjectStreamPtr proteinsStream = directoryObjectStream(directory, T("*.protein"));
#ifdef JUCE_DEBUG
  ObjectContainerPtr res = proteinsStream->load(maxCount ? maxCount : 7)->randomize();
#else
  ObjectContainerPtr res = proteinsStream->load(maxCount)->randomize();
#endif
  for (size_t i = 0; i < res->size(); ++i)
  {
    ProteinPtr protein = res->getAndCast<Protein>(i);
    jassert(protein);
    protein->computeMissingFields();
  }
  return res;
}

class MyResidueFeatures : public ProteinResidueFeatures
{
public:
  virtual String getName() const
    {return T("MyResidueFeatures");}

  virtual featureGenerator compute(ProteinPtr protein, size_t position)
  {
    featureCall(0) computePSSMEntropy(protein, position);    
  }

  FeatureGeneratorPtr computePSSMEntropy(ProteinPtr protein, size_t position)
  {
    ScoreVectorSequencePtr pssm = protein->getPositionSpecificScoringMatrix();
    if (!pssm)
      return FeatureGeneratorPtr();

    double entropy = 0.0;
    for (size_t i = 0; i < pssm->getNumScores(); ++i)
    {
      double probability = pssm->getScore(position, i);
      if (probability > 0.00001)
        entropy -= probability * log2(probability);
    }
    return numberLogFeatures(entropy, 1);
  }
};

class MyResiduePairFeatures : public ProteinResiduePairFeatures
{
public:
  featureGenerator pssmProductFeatures(ScoreVectorSequencePtr pssm, int firstPosition, int secondPosition)
  {
    size_t n = pssm->size();
    if (firstPosition < 0 || firstPosition >= (int)n || secondPosition < 0 || secondPosition >= (int)n)
      return;

    for (size_t i = 0; i < pssm->getNumScores(); ++i)
    {
      double s = pssm->getScore(firstPosition, i) * pssm->getScore(secondPosition, i);
      if (s)
        featureSense(i, s);
    }
  }

  featureGenerator pssmWindowProduct(ScoreVectorSequencePtr pssm, size_t firstPosition, size_t secondPosition, int pssmProductHalfWindowSize)
  {
    for (int delta1 = -pssmProductHalfWindowSize; delta1 <= pssmProductHalfWindowSize; ++delta1)
      featureScope(delta1 + pssmProductHalfWindowSize)
        for (int delta2 = -pssmProductHalfWindowSize; delta2 <= pssmProductHalfWindowSize; ++delta2)
          featureCall(delta2 + pssmProductHalfWindowSize)
            pssmProductFeatures(pssm, (int)firstPosition + delta1, (int)secondPosition + delta2);
  }

  featureGenerator structureNeighborhoodFeatures(ProteinPtr protein, ScoreSymmetricMatrixPtr contactMap, size_t position)
  {
    size_t n = contactMap->getDimension();
    size_t numContacts = 0;
    for (size_t i = 0; i < n; ++i)
      if (contactMap->hasScore(position, i) && i != position)
      {
        if (contactMap->getScore(position, i) > 0.5)
        {
          featureSense(protein->getAminoAcidSequence()->getIndex(i));
          ++numContacts;
        }
      }
    featureCall(0) numberLogFeatures((double)numContacts);
  }

  featureGenerator structureFeatures(ProteinPtr protein, ScoreSymmetricMatrixPtr contactMap, size_t firstPosition, size_t secondPosition)
  {
    featureCall(0) structureNeighborhoodFeatures(protein, contactMap, firstPosition);
    featureCall(1) structureNeighborhoodFeatures(protein, contactMap, secondPosition);
  }

  FeatureGeneratorPtr prolongedDirectionContactFeatures(ScoreSymmetricMatrixPtr contactMap, size_t firstPosition, size_t secondPosition, int delta1, int delta2)
  {
    int dim = (int)contactMap->getDimension();
    size_t count;
    for (count = 0; true; ++count)
    {
      int p1 = (int)firstPosition + delta1 * count;
      int p2 = (int)secondPosition + delta2 * count;
      if (p1 < 0 || p2 < 0 || p1 >= dim || p2 >= dim)
        break;
      if (contactMap->getScore(p1, p2) <= 0.5)
        break;
    }
    return numberLogFeatures(count, 5);
  }

  featureGenerator prolongedContactFeatures(ProteinPtr protein, ScoreSymmetricMatrixPtr contactMap, size_t firstPosition, size_t secondPosition)
  {
    featureCall(0) prolongedDirectionContactFeatures(contactMap, firstPosition, secondPosition, -1, -1);
    featureCall(1) prolongedDirectionContactFeatures(contactMap, firstPosition, secondPosition, -1, +1);
    featureCall(2) prolongedDirectionContactFeatures(contactMap, firstPosition, secondPosition, +1, +1);
    featureCall(3) prolongedDirectionContactFeatures(contactMap, firstPosition, secondPosition, +1, -1);
  }

  virtual String getName() const
    {return T("MyResiduePairFeatures");}

  virtual featureGenerator compute(ProteinPtr protein, size_t firstPosition, size_t secondPosition)
  {
    featureSense("unit");
/*
    ScoreSymmetricMatrixPtr contactMap = protein->getResidueResidueContactMatrix8Ca();

    ScoreVectorSequencePtr pssm = protein->getPositionSpecificScoringMatrix();

    featureCall(1) pssmWindowProduct(pssm, firstPosition, secondPosition, 1);
    
    if (contactMap)
    {
      //featureCall(2) structureFeatures(protein, contactMap, firstPosition, secondPosition);
      featureCall(3) prolongedContactFeatures(protein, contactMap, firstPosition, secondPosition);
    }
     
    //featureCall(1)
//      pssmProductFeatures(protein->getPositionSpecificScoringMatrix(), firstPosition, secondPosition);

    featureScope(0)
    {
      LabelSequencePtr aminoAcidSequence = protein->getAminoAcidSequence();
      featureScope(aminoAcidSequence->getIndex(firstPosition))
        featureSense(aminoAcidSequence->getIndex(secondPosition));
    }*/
  }
};

int main(int argc, char** argv)
{
  declareProteinClasses();

//  File modelDirectory(T("C:\\Projets\\LBC++\\projects\\temp\\Models\\RR.model"));

  File smallPDBDirectory(T("C:\\Projets\\LBC++\\projects\\temp\\SmallPDB"));
   //File smallPDBDirectory(T("/Users/francis/tmp/SmallPDB"));
  //ObjectContainerPtr smallPDBProteins = loadProteins(smallPDBDirectory.getChildFile(T("protein")), 2);
  //std::cout << "SmallPDB " << ProteinStatisticsCalculator::computeStatistics(smallPDBProteins) << std::endl;
  //smallPDBProteins = smallPDBProteins->apply(new ProteinToInputOutputPairFunction());
  
  ObjectContainerPtr smallPDBProteins = directoriesObjectPairStream(
        smallPDBDirectory.getChildFile(T("proteinWithSS3DR")),
        smallPDBDirectory.getChildFile(T("protein")))->load(7);

  ObjectContainerPtr trainingData = smallPDBProteins->size() >= 7 ? smallPDBProteins->invFold(0,7) : smallPDBProteins;
  ObjectContainerPtr testingData = smallPDBProteins->size() >= 7 ? smallPDBProteins->fold(0,7) : smallPDBProteins;

  std::cout << trainingData->size() << " Training Proteins "
            << testingData->size() << " Testing Proteins" << std::endl;


  /*
  ** Creation of the feature function
  */
  CompositeProteinResidueFeaturesPtr featureFunction = new CompositeProteinResidueFeatures();

  //featureFunction->addSubFeatures(proteinUnitResidueFeature());
  //featureFunction->addSubFeatures(proteinPositionIndexResidueFeature()); // DEBUG !!

  //featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("AminoAcidSequence"), 8, 8, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("PositionSpecificScoringMatrix"), 8, 8, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("SecondaryStructureSequence"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("DSSPSecondaryStructureSequence"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("SolventAccessibilityThreshold20"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("DisorderProbabilitySequence"), 5, 5, true));
  featureFunction->addSubFeatures(proteinSequenceWindowFeatures(T("BackboneBondSequence"), 5, 5, true));

  // New features :
  featureFunction->addSubFeatures(proteinPositionFeatures());
  //featureFunction->addSubFeatures(proteinSegmentConjunctionFeatures(T("SecondaryStructureSequence"), 5));

  featureFunction->addSubFeatures(new MyResidueFeatures());

  CompositeProteinResiduePairFeaturesPtr pairFeatureFunction = new CompositeProteinResiduePairFeatures();
  pairFeatureFunction->addSubFeatures(proteinUnitResiduePairFeature());  
//  pairFeatureFunction->addSubFeatures(proteinPositionIndexResiduePairFeature());
  pairFeatureFunction->addSubFeatures(separationLengthResiduePairFeatures());

  // point features
  pairFeatureFunction->addSubFeatures(proteinPointResiduePairFeatures(featureFunction));
  // global
  pairFeatureFunction->addSubFeatures(proteinGlobalToResiduePairFeatures(proteinLengthFeatures(3)));
  pairFeatureFunction->addSubFeatures(proteinGlobalToResiduePairFeatures(proteinGlobalCompositionFeatures(T("AminoAcidSequence"))));
  pairFeatureFunction->addSubFeatures(proteinGlobalToResiduePairFeatures(proteinGlobalCompositionFeatures(T("PositionSpecificScoringMatrix"))));
  pairFeatureFunction->addSubFeatures(proteinGlobalToResiduePairFeatures(proteinGlobalCompositionFeatures(T("SecondaryStructureSequence"))));
  pairFeatureFunction->addSubFeatures(proteinGlobalToResiduePairFeatures(proteinGlobalCompositionFeatures(T("DSSPSecondaryStructureSequence"))));
  pairFeatureFunction->addSubFeatures(proteinGlobalToResiduePairFeatures(proteinGlobalCompositionFeatures(T("DisorderProbabilitySequence"))));


  // central composition features
  pairFeatureFunction->addSubFeatures(proteinCentralCompositionResiduePairFeatures(T("AminoAcidSequence")));
  pairFeatureFunction->addSubFeatures(proteinCentralCompositionResiduePairFeatures(T("PositionSpecificScoringMatrix")));
  pairFeatureFunction->addSubFeatures(proteinCentralCompositionResiduePairFeatures(T("SecondaryStructureSequence")));
  pairFeatureFunction->addSubFeatures(proteinCentralCompositionResiduePairFeatures(T("DisorderProbabilitySequence")));

//  pairFeatureFunction->addSubFeatures(new MyResiduePairFeatures());

  CompositeProteinResiduePairFeaturesPtr pairFeatureFunction2 = new CompositeProteinResiduePairFeatures();
  pairFeatureFunction2->addSubFeatures(proteinUnitResiduePairFeature());  
  pairFeatureFunction2->addSubFeatures(conjunctionResiduePairFeatures(aaCategoryResiduePairConjunction, pairFeatureFunction));
  pairFeatureFunction2->addSubFeatures(conjunctionResiduePairFeatures(proteinLengthResiduePairConjunction, pairFeatureFunction));

  //pairFeatureFunction->addSubFeatures(new TriDimProteinResiduePairFeatures());
  //pairFeatureFunction->addSubFeatures(proteinPositionIndexResiduePairFeature());

  /*
  ** Creation of the inference 
  */
  ProteinInferencePtr proteinInference = new ProteinInference();
//  proteinInference->setPDBDebugDirectory(File(T("C:\\Projets\\LBC++\\projects\\temp\\pdbs")));

  for (size_t i = 0; i < 1; ++i)
  {
    //Protein2DInferenceStepPtr step = new ProteinContactMapInferenceStep(T("RR Pass ") + lbcpp::toString(i),
    //                                                   pairFeatureFunction2, T("ResidueResidueContactMatrix8Ca"));
    //proteinInference->appendStep(step);
    
    proteinInference->appendStep(new ProteinSequenceLabelingInferenceStep(T("SS3 Pass ") + lbcpp::toString(i), featureFunction, /*T("SecondaryStructureProbabilities"), */T("SecondaryStructureSequence")));
#if 0
      step = new ProteinSequenceLabelingInferenceStep(T("DR Pass ") + lbcpp::toString(i), featureFunction, T("DisorderProbabilitySequence"), T("DisorderSequence"));
      proteinInference->appendStep(step);
      step = new ProteinSequenceLabelingInferenceStep(T("SA Pass ") + lbcpp::toString(i), featureFunction, T("SolventAccessibilityThreshold20"));
      proteinInference->appendStep(step);
      step = new ProteinBackboneBondSequenceInferenceStep(T("BBB Pass ") + lbcpp::toString(i), featureFunction);
      proteinInference->appendStep(step);
#endif // 0
  }
  //std::cout << "Inference: " << proteinInference->toString() << std::endl;

  /*
  ** Learning
  */
  InferenceLearnerCallbackPtr callback = new MyInferenceLearnerCallback(trainingData, testingData, false);
  //InferenceLearnerPtr learner = stepByStepDeterministicSimulationLearner(callback, true);//, modelDirectory, true);
  InferenceLearnerPtr learner = globalSimulationLearner(callback);
  learner->train(proteinInference, trainingData);

  /*
  ** Evaluation
  *
  InferenceContextPtr validationContext = singleThreadedInferenceContext();
  ProteinEvaluationCallbackPtr evaluation = new ProteinEvaluationCallback();
  validationContext->appendCallback(evaluation);

  validationContext->runWithSupervisedExamples(proteinInference, trainingData);
  std::cout << "Train evaluation: " << evaluation->toString() << std::endl;

  validationContext->runWithSupervisedExamples(proteinInference, testingData);
  std::cout << "Test evaluation: " << evaluation->toString() << std::endl;
  */
  return 0;
}
